{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019komazawa_NST_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_NST_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrHeSDS2KuNe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "    <font  style=\"font-weight: bolder; \" color=\"darkgreen\" size=\"+2\" > 2019年度 駒澤大学心理学特講IIIA 演習教材</font>\n",
        "</p>\n",
        "\n",
        "# 絵師になろう :-)  ニューラル画風変換 neural style transfer の実習\n",
        "\n",
        "\n",
        "<p align=\"right\">\n",
        "<a href=\"mailto:asakawa@ieee.org\"><font size=\"+0\" style=\"font-weight:bold\" color=\"RoyalBlue\"> 浅川伸一</font></a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r9I6UNTO16r",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## はじめに\n",
        "\n",
        "[Gatys et al., (2015)](https://arxiv.org/abs/1508.06576) によって提唱された画風変換を紹介します。<font color=\"green\">ちなみに第一著者の Gatys は発表当時博士課程の学生</font>でした。\n",
        "\n",
        "彼の考案した画風変換方法は反響を呼び，画像生成，アーティストの関心を集めました。その証拠に [国際会議でコンテンスト](https://nips2017creativity.github.io/) が開催されるほどになりました。\n",
        "画像生成技術は，[Ian GoodfelIow](https://twitter.com/goodfellow_ian?lang=en)  の [GAN](https://arxiv.org/abs/1406.2661) や [Durk Kingma](https://twitter.com/dpkingma)  の  [VAE](https://arxiv.org/abs/1312.6114)  とともに，技術的な意味ばかりでなく，ニューラルネットワーク内部で何が表現されているのかを考える材料でもあり，心理学者の注目すべき技術であると考えます。(<font color=\"green\">ちなみに Goodfellow も Kingma も論文発表当時博士課程の学生でした</font>)\n",
        "\n",
        "この文章は，[Harish Narayanan](https://twitter.com/copingbear) の [ブログ](https://harishnarayanan.org/writing/artistic-style-transfer/) と彼の公開した [コード](https://github.com/hnarayanan/artistic-style-transfer)  に基づいています。すぐれたコードを公開してくれた彼に敬意を表します。\n",
        "\n",
        "画風変換については [Justin Johnson](https://twitter.com/jcjohnss)  が [高速化アルゴリズム](https://arxiv.org/abs/1603.08155)  を発表しだれでもが楽しめる下地ができました。(<font color=\"green\">ちなみに彼もスタンフォード大学の博士課程の学生</font>です。スタンフォード大学の畳込みニューラルネットワークによる画像認識の授業  [cs231](http://cs231n.stanford.edu/) のチューターを務めています。\n",
        "\n",
        "画像変換のポイントは，2つの損失関数，場合によっては全体変動損失を含めた 3 つの損失関数の和として損失関数を定義することにあります。\n",
        "\n",
        "1. 内容損失 content loss\n",
        "2. スタイル損失 style loss\n",
        "3. 全体変動損失 total variation loss\n",
        "\n",
        "全体変動損失は用いられない場合もあります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMVIpZCi-b42",
        "colab_type": "text"
      },
      "source": [
        "# 準備作業\n",
        "\n",
        "必要なライブラリの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUSvh3p17CzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkFopO7ETTPZ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "内容画像とスタイル画像を読み込みます。\n",
        "必要に応じて自分の画像に置き換えることができます。\n",
        "\n",
        "Colab では，Google drive からデータを読み込むために，最初に認証を行う必要があるため，以下の作業が必要になります。\n",
        "画面に表示される指示に従い，自分の Google アカウントを用いて認証を行ってください。\n",
        "別タブが開いて認証キーが表示されますので，そのキーを貼り付けてエンターキーを押してください"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvV8Ia5r7Dm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwyw4-Ow7M5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 実習のための画像データの ID を入れて，データを入手\n",
        "download = drive.CreateFile({'id': '1qETyedlJmViOWPUo3fWdsbtk8hsmJPiZ'})\n",
        "download.GetContentFile('testimages.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBUGup-p_L7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls *.zip\n",
        "!unzip testimages.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTnmYbNf_kRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 授業中のデモのため画像を小さくして利用しています。時間節約のため\n",
        "height = 128\n",
        "width =128\n",
        "\n",
        "content_image_path = 'withElman.jpg'\n",
        "content_image = Image.open(content_image_path)\n",
        "content_image = content_image.resize((width, height))\n",
        "content_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0fWRFfAKfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_image_path = 'shinkai.jpg'\n",
        "style_image = Image.open(style_image_path)\n",
        "style_image = style_image.resize((width, height))\n",
        "style_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsiratC3AXXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_array = np.asarray(content_image, dtype='float32')\n",
        "content_array = np.expand_dims(content_array, axis=0)\n",
        "print(content_array.shape)\n",
        "\n",
        "style_array = np.asarray(style_image, dtype='float32')\n",
        "style_array = np.expand_dims(style_array, axis=0)\n",
        "print(style_array.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZrAvpcaAcKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_array[:, :, :, 0] -= 103.939\n",
        "content_array[:, :, :, 1] -= 116.779\n",
        "content_array[:, :, :, 2] -= 123.68\n",
        "content_array = content_array[:, :, :, ::-1]\n",
        "\n",
        "style_array[:, :, :, 0] -= 103.939\n",
        "style_array[:, :, :, 1] -= 116.779\n",
        "style_array[:, :, :, 2] -= 123.68\n",
        "style_array = style_array[:, :, :, ::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkJTwL-EQ-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_image = backend.variable(content_array)\n",
        "style_image = backend.variable(style_array)\n",
        "combination_image = backend.placeholder((1, height, width, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XtxpT1hIn11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor = backend.concatenate([content_image,\n",
        "                                    style_image,\n",
        "                                    combination_image], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiXDsaT8I0ng",
        "colab_type": "text"
      },
      "source": [
        "## 事前学習されたモデルの読み込みと損失関数の定義\n",
        "\n",
        "The core idea introduced by [Gatys et al. (2015)](https://arxiv.org/abs/1508.06576) is that convolutional neural networks (CNNs) pre-trained for image classification already know how to encode perceptual and semantic information about images. We're going to follow their idea, and use the *feature spaces* provided by one such model to independently work with content and style of images.\n",
        "\n",
        "The original paper uses the 19 layer VGG network model from [Simonyan and Zisserman (2015)](https://arxiv.org/abs/1409.1556), but we're going to instead follow [Johnson et al. (2016)](https://arxiv.org/abs/1603.08155) and use the 16 layer model (VGG16). There is no noticeable qualitative difference in making this choice, and we gain a tiny bit in speed.\n",
        "\n",
        "It is trivial for us to get access to this truncated model because Keras comes with a set of pretrained models, including the VGG16 model we're interested in. Note that by setting `include_top=False` in the code below, we don't include any of the fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCDp6EPQIrCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16(input_tensor=input_tensor, weights='imagenet',\n",
        "              include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq0Py4XvJDcW",
        "colab_type": "text"
      },
      "source": [
        "各層の表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz0BEcKDI_kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCcPXzkJJFrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content_weight = 0.025\n",
        "style_weight = 5.0\n",
        "total_variation_weight = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_UV8zFfJM7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = backend.variable(0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRN9N5J4JV-2",
        "colab_type": "text"
      },
      "source": [
        "### 内容損失\n",
        "\n",
        "For the content loss, we follow Johnson et al. (2016) and draw the content feature from `block2_conv2`, because the original choice in Gatys et al. (2015) (`block4_conv2`) loses too much structural detail. And at least for faces, I find it more aesthetically pleasing to closely retain the structure of the original content image.\n",
        "\n",
        "This variation across layers is shown for a couple of examples in the images below (just mentally replace `reluX_Y` with our Keras notation `blockX_convY`).\n",
        "\n",
        "<!--#![Content feature reconstruction](images/content-feature.png \"Content feature reconstruction\")\n",
        "-->\n",
        "The content loss is the (scaled, squared) Euclidean distance between feature representations of the content and combination images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPpXnj0zJSEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def content_loss(content, combination):\n",
        "    return backend.sum(backend.square(combination - content))\n",
        "\n",
        "layer_features = layers['block2_conv2']\n",
        "content_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "\n",
        "loss += content_weight * content_loss(content_image_features,\n",
        "                                      combination_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrvwwIGaJhqe",
        "colab_type": "text"
      },
      "source": [
        "### スタイル損失\n",
        "\n",
        "画風変換のコアアイデアですが，グラム行列を定義します。通常の損失関数との相違を確認してください\n",
        "\n",
        "For the style loss, we first define something called a *Gram matrix*. The terms of this matrix are proportional to the covariances of corresponding sets of features, and thus captures information about which features tend to activate together. By only capturing these aggregate statistics across the image, they are blind to the specific arrangement of objects inside the image. This is what allows them to capture information about style independent of content. (This is not trivial at all, and I refer you to [a paper that attempts to explain the idea](https://arxiv.org/abs/1606.01286).)\n",
        "\n",
        "The Gram matrix can be computed efficiently by reshaping the feature spaces suitably and taking an outer product.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2unOo7rJe9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(x):\n",
        "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = backend.dot(features, backend.transpose(features))\n",
        "    return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BH9md72JzaG",
        "colab_type": "text"
      },
      "source": [
        "The style loss is then the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.\n",
        "\n",
        "Again, in the following code, I've chosen to go with the style features from layers defined in Johnson et al. (2016) rather than Gatys et al. (2015) because I find the end results more aesthetically pleasing. I encourage you to experiment with these choices to see varying results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKOhIqwbJkuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_loss(style, combination):\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = height * width\n",
        "    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "feature_layers = ['block1_conv2', 'block2_conv2',\n",
        "                  'block3_conv3', 'block4_conv3',\n",
        "                  'block5_conv3']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = layers[layer_name]\n",
        "    style_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_features, combination_features)\n",
        "    loss += (style_weight / len(feature_layers)) * sl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVWDjhrOJ6JG",
        "colab_type": "text"
      },
      "source": [
        "### 全体変動損失\n",
        "\n",
        "Poggio らの標準正則化理論との関連に注目してください\n",
        "\n",
        "Now we're back on simpler ground.\n",
        "\n",
        "If you were to solve the optimisation problem with only the two loss terms we've introduced so far (style and content), you'll find that the output is quite noisy. We thus add another term, called the [total variation loss](http://arxiv.org/abs/1412.0035) (a regularisation term) that encourages spatial smoothness.\n",
        "\n",
        "You can experiment with reducing the `total_variation_weight` and play with the noise-level of the generated image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn7y2YpEJ2y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_variation_loss(x):\n",
        "    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n",
        "    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n",
        "    return backend.sum(backend.pow(a + b, 1.25))\n",
        "\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdfKhfOpKA_Q",
        "colab_type": "text"
      },
      "source": [
        "## 損失関数が定義できたので，これを用いて最適化問題を解くことを考えます\n",
        "\n",
        "The goal of this journey was to setup an optimisation problem that aims to solve for a *combination image* that contains the content of the content image, while having the style of the style image. Now that we have our input images massaged and our loss function calculators in place, all we have left to do is define gradients of the total loss relative to the combination image, and use these gradients to iteratively improve upon our combination image to minimise the loss.\n",
        "\n",
        "We start by defining the gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3AGZ4OJ9OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grads = backend.gradients(loss, combination_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so_xMqPHKOK9",
        "colab_type": "text"
      },
      "source": [
        "各エポックで損失関数を評価し，その評価した値にしたがって勾配降下法を実行します。\n",
        "\n",
        "We then introduce an `Evaluator` class that computes loss and gradients in one pass while retrieving them via two separate functions, `loss` and `grads`. This is done because `scipy.optimize` requires separate functions for loss and gradients, but computing them separately would be inefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fW0u4eFKJjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = [loss]\n",
        "outputs += grads\n",
        "f_outputs = backend.function([combination_image], outputs)\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    x = x.reshape((1, height, width, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1].flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values\n",
        "\n",
        "evaluator = Evaluator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXxCR8ReKUt9",
        "colab_type": "text"
      },
      "source": [
        "L-BFGS を使っていますが，ここでは拘らなくて良いです。\n",
        "簡単のため 10 回だけ評価しています。\n",
        "\n",
        "Now we're finally ready to solve our optimisation problem. This combination image begins its life as a random collection of (valid) pixels, and we use the [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) algorithm (a quasi-Newton algorithm that's significantly quicker to converge than standard gradient descent) to iteratively improve upon it.\n",
        "\n",
        "We stop after 10 iterations because the output looks good to me and the loss stops reducing significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb1jJ8FNKRe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.\n",
        "\n",
        "iterations = 10\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    end_time = time.time()\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DYBSyYWKbJK",
        "colab_type": "text"
      },
      "source": [
        "結果の表示までに時間がかかります。気長に待ってください\n",
        "\n",
        "This took a while on my piddly laptop (that isn't GPU-accelerated), but here is the beautiful output from the last iteration! (Notice that we need to subject our output image to the inverse of the transformation we did to our input images before it makes sense.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqryyBT2KXp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = x.reshape((height, width, 3))\n",
        "x = x[:, :, ::-1]\n",
        "x[:, :, 0] += 103.939\n",
        "x[:, :, 1] += 116.779\n",
        "x[:, :, 2] += 123.68\n",
        "x = np.clip(x, 0, 255).astype('uint8')\n",
        "\n",
        "Image.fromarray(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAV5B1VAaUFX",
        "colab_type": "text"
      },
      "source": [
        "- That's it.  Enjoy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6XjxDfTaZVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}