{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019komazawa_CycleGAN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/2019komazawa/blob/master/notebooks/2019komazawa_CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj_BcANlXq6p",
        "colab_type": "text"
      },
      "source": [
        "# Generative Image to Image Translation with CycleGAN  \n",
        "[Parag K. Mital](https://pkmital.com)  \n",
        "[Creative Applications of Deep Learning](https://www.kadenze.com/programs/creative-applications-of-deep-learning-with-tensorflow)  \n",
        "[Kadenze, Inc.](https://kadenze.com)  \n",
        "\n",
        "This content appears as part of the course, [Creative Applications of Deep Learning](https://www.kadenze.com/programs/creative-applications-of-deep-learning-with-tensorflow), as part of the [Kadenze Academy](https://kadenze.com) program.  This content is licensed under an [APL 2.0 License](https://github.com/pkmital/CycleGAN/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHgpVfyNq0Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U scipy==1.2.1\n",
        "# https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/652"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfcpBkH_ZL6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNjXfSCX7ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/pkmital/CycleGAN.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlOAH6kPYuU8",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This colab introduces you to the following work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9sVyzK0YvCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "plt = matplotlib.pyplot\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/cycle-gan-paper.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOxr2bGXq6u",
        "colab_type": "text"
      },
      "source": [
        "Image to image translation covers a very wide set of applications in computer graphics, computer vision, and deep learning with image and video.  The basic idea is to translate an input image into an output image.  This is different to say autoencoding an image, where both the input and the output are exactly the same.  In this case, they are expected to be different sets of images.  For instance, the input image might be a landscape photo and the output could be an artistic stylization of that photo.  Or perhaps the input is a photo of a horse, and the output should be photos of zebras instead.  Let's say instead you have a maps showing the outlines of streets and highways, and you'd like to apply a texture to these images so that it looks like a satellite image instead.  Or maybe you want to recreate the Google AI Experiment where they convert sketches to pictures of cats.  Or perhaps you want to recreate the app FaceApp which adds smiles to people's faces.  These are all examples of image to image translation. \n",
        "\n",
        "One of the earliest demonstrations of this idea is in a paper in 2001 called Image Analogies.  Aaron Hertzmann and colleagues showed the basic idea of image to image translation by using analogous image pairs.  You would give an example analogy, such as an image of a street map, and the corresponding satellite image, and then you could give it any other street map image, and it would give you a new satellite image for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOgK9f44ZUyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/image-analogies-paper.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVSlKbcuZVSr",
        "colab_type": "text"
      },
      "source": [
        "How could we build a neural network to do something similar?  Autoencoders and generative adversarial networks allow for some pretty impressive unsupervised modeling of image collections.  For instance, an autoencoder with an input different from the output could essentially learn to translate one image into another.  Let's say for instance if you had black and white images, and wanted to convert them to color images, you could create a dataset of either image, and then feed in pairs of each type of image to an autoencoder.  The resulting loss function would then take as output some color image, and input a black and white image, and the network would need to learn to essentially colorize the image.\n",
        "\n",
        "Though if you've ever tried this, you might have found that autoencoders generally have a pretty big issue with images: their loss function generally uses a L2 or squared loss function, and this often results in really blurry reconstructions, or reconstructions that don't necessarily understand the content of the image beyond simple pixels.  \n",
        "\n",
        "We then saw that the generative adversarial network did not have this issue since it learned its own loss function by training a separate network, a discriminator, to say whether an image was real or fake.   So could we build a generative adversarial network that could learn to colorize an image?  Or to convert street maps into satellite maps?  Or any other potential application of image to image translation?  This tutorial covers one of the state of the art techniques for image to image translation called CycleGAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g7-jRboXq6u",
        "colab_type": "text"
      },
      "source": [
        "# CycleGAN\n",
        "\n",
        "CycleGAN builds on earlier work called Pix2Pix.  The Pix2Pix network requires paired translations, which means that while training, for each input, you need to specify exactly what the output should look like.  CycleGAN instead just requires two _unpaired_ collections of images and will do its best to find the mapping between them, without you having to specify what the pairs are.  It's a really impressive network and pretty simple to build.  We'll need many of the same components that we needed for the DCGAN and VAEGAN networks we built in Course 1 Session 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8daR2toNZn3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/cycle-gan-figure-2.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qshOtt8QZoc_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We're going to need to build a set of operations that map one image collection to another, and build those operations for each image collection $X$ and $Y$.  We'll also have to build another set of operations so that there is a complete cycle of operations that attempt to map our first collection of images, $X$, to the second collection, $Y$, so a fake $Y$ which we denote mathematically by $\\hat{Y}$, and then another generator to map both the real $Y$ and the fake $\\hat{Y}$ back to $\\hat{X}$ again. In the Figure below, we can sort of see how this works graphically with two collections, $X$ and $Y$:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnltoOyPZqre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/cycle-gan-figure-3.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrJNZ-5oXq6u",
        "colab_type": "text"
      },
      "source": [
        "# Quiz:\n",
        "\n",
        "> How many generative networks are in the CycleGAN network?\n",
        "\n",
        "> How many discriminators are in the CycleGAN network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lk3A5mXq6v",
        "colab_type": "text"
      },
      "source": [
        "# Encoder\n",
        "\n",
        "This network essentially trains 2 autoencoders using a set of 2 generators (`G` and `F` in the images above) and 2 discriminators (`D_X` and `D_Y` in the images above) .  The idea is pretty easy to grasp if you are already familiar with generative adversarial networks, and most of the tricky bits are in the details of implementation.\n",
        "\n",
        "The Generator is slightly different though.  Instead of starting with a random feature vector of say 100 values, we'll actually start with an image and then compose something not unlike an Autoencoder.  The structure of this Autoencoder is a little different.  We'll have three major components: an encoder, transformer, and decoder. \n",
        "\n",
        "The encoder is composed of a few convolutional layers with stride 2 which will downsample the image with each layer.  The authors create it with 3 layers, and use padding on the first layer.\n",
        "\n",
        "Let's see it written up as code.  First some imports.  We'll include TensorFlow as well as a contrib package which makes writing layers a lot easier, similar to Keras.  We'll also be using Instance Normalization (http://arxiv.org/abs/1607.08022) for our layer normalization and Leaky ReLus as the authors of CycleGAN have done.  I've included my implementation of CycleGAN and all utility functions in the `cycle_gan.py` module which you can also find as part of the `cadl` repo: https://github.com/pkmital/pycadl (easily pip installed by: `pip install cadl`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hipr97jtXq6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cadl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.layers as tfl\n",
        "from cadl.cycle_gan import lrelu, instance_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9iz_ijaXq61",
        "colab_type": "text"
      },
      "source": [
        "Now let's write a function for the encoder, following the CycleGAN architecture.  We want a padding layer, then 3 convolutional layers with strides 1, then 2, then another 2.  The first convolutional layer will have a 7x7 convolution, and the rest will be 3x3.  Also, we'll initialize the weights to a standard deviation of 0.02 using a normal distribution.  For an activation function, we'll be using a Leaky ReLu with 0.2 leakage.  We'll also exponentially increase the number of filters with each layer, starting with 32, then going to 64 and finally 128.  Lastly, we'll also use something like batch normalization called instance normalization, and use the TensorFlow layers module to do the entire convolution operation for us in a single convenient function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7xUHe93Xq61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(x, n_filters=32, k_size=3, normalizer_fn=instance_norm,\n",
        "        activation_fn=lrelu, scope=None, reuse=None):\n",
        "    with tf.variable_scope(scope or 'encoder', reuse=reuse):\n",
        "        h = tf.pad(x, [[0, 0], [k_size, k_size], [k_size, k_size], [0, 0]],\n",
        "                \"REFLECT\")\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters,\n",
        "                kernel_size=7,\n",
        "                stride=1,\n",
        "                padding='VALID',\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                activation_fn=activation_fn,\n",
        "                scope='1',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters * 2,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                activation_fn=activation_fn,\n",
        "                scope='2',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters * 4,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                activation_fn=activation_fn,\n",
        "                scope='3',\n",
        "                reuse=reuse)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MxrZ9inXq63",
        "colab_type": "text"
      },
      "source": [
        "We're also being explicit about our scope and reuse as we'll need to reuse these variables a few times which we'll see in a few moments.\n",
        "\n",
        "# Residual Blocks and Transformer\n",
        "\n",
        "The next part of the Generator is the Transformer.  These are going to be 6 or 9 Residual Blocks, which is a really powerful module that comes up a lot in almost every new architecture out there.  Instead of simply having a convolutional layer, we'll have a convolutional layer and then sum together the original output.  So it's a sort of residual function of the input which should be learned, residual meaning what's left over.  The residual blocks allow the base activation to persist, but then learns a simple addition on top of that layer.  This is useful because it ensures the original activation has a path to the output.  And similarly, it is useful for backpropagation since the gradient has less chance of exploding or vanishing, as they typically can do in very deep networks.  To read more about residual networks, check the original paper which shows how to create a network of 1000s of layers, all without having issues with vanishing or exploding gradients!\n",
        "\n",
        "Alright let's code up the residual block.  All the convolutions are going to be single stride, and 128 channels.  Each block will have a pad layer, a 3x3 convolution with Leaky ReLu and Instance Normalization, another pad layer, another convolution with Instance Normalization, except no nonlinearity, and then an addition with the starting activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7brerWPXq63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x, n_channels=128, normalizer_fn=instance_norm,\n",
        "        activation_fn=lrelu, kernel_size=3, scope=None, reuse=None):\n",
        "    with tf.variable_scope(scope or 'residual', reuse=reuse):\n",
        "        h = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                padding='VALID',\n",
        "                activation_fn=activation_fn,\n",
        "                scope='1',\n",
        "                reuse=reuse)\n",
        "        h = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                padding='VALID',\n",
        "                activation_fn=None,\n",
        "                scope='2',\n",
        "                reuse=reuse)\n",
        "        h = tf.add(x, h)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfTfkYoCXq65",
        "colab_type": "text"
      },
      "source": [
        "Now we can compose many residual blocks to create our Transformer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7bO7TqhXq67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(x, img_size=256, reuse=None):\n",
        "    h = x\n",
        "    if img_size >= 256:\n",
        "        n_blocks = 9\n",
        "    else:\n",
        "        n_blocks = 6\n",
        "    for block_i in range(n_blocks):\n",
        "        with tf.variable_scope('block_{}'.format(block_i), reuse=reuse):\n",
        "            h = residual_block(h, reuse=reuse)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As_q8F3sXq68",
        "colab_type": "text"
      },
      "source": [
        "# Decoder\n",
        "\n",
        "Great, now the last piece we need to code up our Generator is the Decoder.  This is basically going to do the complete opposite of our Encoder.  We'll have three deconvolutional layers with stride 2, stride 2, and stride 1, and kernel sizes 3, 3, and 7.  Before the last layer we'll also pad to avoid boundary artifacts with the larger 7x7 kernel, and our last activation will be a tanh, meaning our images will be in the range of -1 and 1.  Generally this is an ideal normalization for images as it means the starting point is basically a grey image.  We'll need to keep this in mind when we feed our data into the network and ensure we are using images in the same range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k__p_96OXq68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(x, n_filters=32, k_size=3, normalizer_fn=instance_norm,\n",
        "        activation_fn=lrelu, scope=None, reuse=None):\n",
        "    with tf.variable_scope(scope or 'decoder', reuse=reuse):\n",
        "        h = tfl.conv2d_transpose(\n",
        "                inputs=x,\n",
        "                num_outputs=n_filters * 2,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                activation_fn=activation_fn,\n",
        "                scope='1',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d_transpose(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                activation_fn=activation_fn,\n",
        "                scope='2',\n",
        "                reuse=reuse)\n",
        "        h = tf.pad(h, [[0, 0], [k_size, k_size], [k_size, k_size], [0, 0]],\n",
        "                \"REFLECT\")\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=3,\n",
        "                kernel_size=7,\n",
        "                stride=1,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                padding='VALID',\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                activation_fn=tf.nn.tanh,\n",
        "                scope='3',\n",
        "                reuse=reuse)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kviE1DIWXq6_",
        "colab_type": "text"
      },
      "source": [
        "Putting it all together, our Generator will first encode, then transform, and then finally decode like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEawYyWFXq6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(x, scope=None, reuse=None):\n",
        "    img_size = x.get_shape().as_list()[1]\n",
        "    with tf.variable_scope(scope or 'generator', reuse=reuse):\n",
        "        h = encoder(x, reuse=reuse)\n",
        "        h = transform(h, img_size, reuse=reuse)\n",
        "        h = decoder(h, reuse=reuse)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1twuwTThXq7C",
        "colab_type": "text"
      },
      "source": [
        "# PatchGAN, Receptive Field Sizes, and the Discriminator\n",
        "\n",
        "The other major component is the Discriminator.  This network will take as input an image and then output a single value.  In the case of a true image, it should output 1, and in the case of a false image, it should output 0.  For the generator, we want the opposite to be true.  In any case, the discriminator should saturate at 0 or 1, so will need a sigmoid as its final activation.  The network will take as input a 256 x 256 pixel image and use a series of 5 convolutional layers not unlike the ones we've already used.  The first three layers will be stride 2, and then the last 2 will be stride 1.  We'll increase the number of outputs exponentially until the last layer which will have a single channel as output.\n",
        "\n",
        "Unlike a typical GAN, what we're creating is what the Pix2Pix and CycleGAN authors call a PatchGAN discriminator.  This network doesn't actually reduce down the image to a single value, but instead will reduce down the 256 x 256 pixel image to a spatial map with 1 channel as output.  The resulting map effectively has individual discriminators which we average together to get the final result.  The authors show a few possible combinations of stride and layer sizes to get effectively different receptive field sizes in the final layer, and show that this combination of 5 layers seems to have the best performance and a receptive field size of 70.\n",
        "\n",
        "Let's break it down a bit more and see how they come up with a receptive field size of 70:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMSKSo3cahyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/receptive-field-sizes.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HL_xhBuai_B",
        "colab_type": "text"
      },
      "source": [
        "In the image above, we can see the input layer at the top, and the final layer at the bottom.  Working form the final layer back to the top, we can see how 1 neuron contributes to an increasing number of neurons in preceding layers.  The receptive field for each layer for a single neuron in the last layer is written in the right margin: [1, 4, 7, 16, 34, 70]\n",
        "\n",
        "The code for the discriminator looks like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oONZ70XXq7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(x, n_filters=64, k_size=4, activation_fn=lrelu,\n",
        "        normalizer_fn=instance_norm, scope=None, reuse=None):\n",
        "    with tf.variable_scope(scope or 'discriminator', reuse=reuse):\n",
        "        h = tfl.conv2d(\n",
        "                inputs=x,\n",
        "                num_outputs=n_filters,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                activation_fn=activation_fn,\n",
        "                normalizer_fn=None,\n",
        "                scope='1',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters * 2,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                activation_fn=activation_fn,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                scope='2',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters * 4,\n",
        "                kernel_size=k_size,\n",
        "                stride=2,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                activation_fn=activation_fn,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                scope='3',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=n_filters * 8,\n",
        "                kernel_size=k_size,\n",
        "                stride=1,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                activation_fn=activation_fn,\n",
        "                normalizer_fn=normalizer_fn,\n",
        "                scope='4',\n",
        "                reuse=reuse)\n",
        "        h = tfl.conv2d(\n",
        "                inputs=h,\n",
        "                num_outputs=1,\n",
        "                kernel_size=k_size,\n",
        "                stride=1,\n",
        "                weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                biases_initializer=None,\n",
        "                activation_fn=tf.nn.sigmoid,\n",
        "                scope='5',\n",
        "                reuse=reuse)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIsjkvNbXq7E",
        "colab_type": "text"
      },
      "source": [
        "# Connecting the Pieces\n",
        "\n",
        "Now we've got all the major components we need to create a CycleGAN.  We just need to connect them up using a few placeholders, create our loss functions, and finally build our training method.  Let's start with our placeholders.\n",
        "\n",
        "We'll start with placeholders for each of the two collections which I'll call `X` and `Y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNCno0iXq7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = 256\n",
        "X_real = tf.placeholder(name='X', shape=[1, img_size, img_size, 3], dtype=tf.float32)\n",
        "Y_real = tf.placeholder(name='Y', shape=[1, img_size, img_size, 3], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stz2wR8KXq7H",
        "colab_type": "text"
      },
      "source": [
        "To get the \"fake\" outputs of these \"real\" inputs, we give them to a corresponding generator.  We'll have one generator for each direction that we'd like to go in.  One which converts the X style to a Y style, and vice-versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2H3IVbhXq7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_fake = generator(Y_real, scope='G_yx')\n",
        "Y_fake = generator(X_real, scope='G_xy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1SDYY8sXq7L",
        "colab_type": "text"
      },
      "source": [
        "Because this is a CycleGAN, we'll enforce an additional constraint on the generated output to match the original image quality with an L1-Loss.  This will effectively test both generators by generating from X to Y and then back to X again. Similarly, for Y, we'll generate to X, and again to Y.  To get these images, we simple reuse the existing generators and create the cycle images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAOx-LMZXq7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_cycle = generator(Y_fake, scope='G_yx', reuse=True)\n",
        "Y_cycle = generator(X_fake, scope='G_xy', reuse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZXWPQ7zXq7P",
        "colab_type": "text"
      },
      "source": [
        "Our discriminators will then act on the `real` and `fake` images like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcj5uAmnXq7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_X_real = discriminator(X_real, scope='D_X')\n",
        "D_Y_real = discriminator(Y_real, scope='D_Y')\n",
        "D_X_fake = discriminator(X_fake, scope='D_X', reuse=True)\n",
        "D_Y_fake = discriminator(Y_fake, scope='D_Y', reuse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xjDkSnGXq7R",
        "colab_type": "text"
      },
      "source": [
        "To create our generator's loss, we'll compute the L1 distance between the `cycle` and `real` images, and test how well the generator \"fools\" the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-gw_m-WXq7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = 10.0\n",
        "loss_cycle = tf.reduce_mean(l1 * tf.abs(X_real - X_cycle)) + \\\n",
        "             tf.reduce_mean(l1 * tf.abs(Y_real - Y_cycle))\n",
        "loss_G_xy = tf.reduce_mean(tf.square(D_Y_fake - 1.0)) + loss_cycle\n",
        "loss_G_yx = tf.reduce_mean(tf.square(D_X_fake - 1.0)) + loss_cycle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69mnO0l1Xq7U",
        "colab_type": "text"
      },
      "source": [
        "The authors suggest to use a constant weighting on the L1 cycle loss of 10.0.\n",
        "\n",
        "Finally, we'll need to compute the loss for our discriminators.  Unlike the generators which use the current generation of fake images, we'll actually use a history buffer of generated images, and randomly sample a generated image from this history buffer.  Previous work on GANs has shown this can help training and the CycleGAN authors suggest using it as well.  We'll take care of keeping track of this history buffer on the CPU side of things and create a placeholder for the TensorFlow graph to help send the history image into the graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2fZH-MRXq7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_fake_sample = tf.placeholder(name='X_fake_sample',\n",
        "        shape=[None, img_size, img_size, 3], dtype=tf.float32)\n",
        "Y_fake_sample = tf.placeholder(name='Y_fake_sample',\n",
        "        shape=[None, img_size, img_size, 3], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyAZ9q_rXq7X",
        "colab_type": "text"
      },
      "source": [
        "Now we'll ask the discriminator to assess these images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7umcgRVXq7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_X_fake_sample = discriminator(X_fake_sample, scope='D_X', reuse=True)\n",
        "D_Y_fake_sample = discriminator(Y_fake_sample, scope='D_Y', reuse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHKye6YXq7Z",
        "colab_type": "text"
      },
      "source": [
        "And now we can create our loss for the discriminator.  Unlike the original GAN implementation, we use a square loss instead of binary cross entropy loss.  This turns out to be a bit less prone to errors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECN_2JQTXq7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_D_Y = (tf.reduce_mean(tf.square(D_Y_real - 1.0)) + \\\n",
        "            tf.reduce_mean(tf.square(D_Y_fake_sample))) / 2.0\n",
        "loss_D_X = (tf.reduce_mean(tf.square(D_X_real - 1.0)) + \\\n",
        "            tf.reduce_mean(tf.square(D_X_fake_sample))) / 2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8XMPNDgXq7b",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "Let's now take a look at how to build optimizers for such a model.  I've wrapped everything we've just done into a convenient module called `cycle_gan`.  We can create the entire network like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgyo5HhzXq7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "from cadl.cycle_gan import cycle_gan\n",
        "net = cycle_gan(img_size=img_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqbfBothXq7e",
        "colab_type": "text"
      },
      "source": [
        "This will return the entire network in a dict for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh2dtBo1Xq7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(net.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L99Qdbx4Xq7h",
        "colab_type": "text"
      },
      "source": [
        "Just like in the original GAN implementation, we'll create individual optimizers which can only update certain parts of the network.  The original GAN had two optimizers, one for the generator and one for the discriminator.  Even though the discriminator depends on input from the generator, we would only optimize the variables belonging to the discriminator when training the discriminator.  If we did not do this, we'd be making the generator *worse*, when what we really want to happen is for both networks to get better.  We'll do the same thing here, except now we actually have 3 networks to optimize, and so we'll need 3 optimizers: `G_xy` and `G_yx` variables will be optimized as the generator, while `D_X`, and `D_Y`, should update two different discriminators.\n",
        "\n",
        "First let's get the variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ40SI8EXq7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_vars = tf.trainable_variables()\n",
        "D_X_vars = [v for v in training_vars if v.name.startswith('D_X')]\n",
        "D_Y_vars = [v for v in training_vars if v.name.startswith('D_Y')]\n",
        "G_xy_vars = [v for v in training_vars if v.name.startswith('G_xy')]\n",
        "G_yx_vars = [v for v in training_vars if v.name.startswith('G_yx')]\n",
        "G_vars = G_xy_vars + G_yx_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzn3fcSvXq7i",
        "colab_type": "text"
      },
      "source": [
        "And then build the optimizers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNSxur_mXq7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "D_X = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
        "        net['loss_D_X'], var_list=D_X_vars)\n",
        "D_Y = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
        "        net['loss_D_Y'], var_list=D_Y_vars)\n",
        "G = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
        "        net['loss_G'], var_list=G_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qevWFHZsXq7l",
        "colab_type": "text"
      },
      "source": [
        "Note that we concatenated both generators into one variable list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWpHDXVrXq7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHmcQk4yXq7q",
        "colab_type": "text"
      },
      "source": [
        "As part of the discriminator training, we test how it classifies real images and generated images.  For the generated images, the discriminator takes a randomly generated image from the last 50 some generated images.  This is to make the training a bit more stable, according to: Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., & Webb, R. (2016). Learning from Simulated and Unsupervised Images through Adversarial Training. Retrieved from http://arxiv.org/abs/1612.07828 - see Section 2.3 for details.  The idea here is the discriminator should still be able to say that older generated images are fake.  It may be the case that the generator just re-learns things the discriminator has forgotten about, and this might help with making things more stable.\n",
        "\n",
        "To set this up, we determine our `capacity`, such as 50 images, and create a list of images all initialized to 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYoZfqOyXq7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many fake generations to keep around\n",
        "capacity = 50\n",
        "\n",
        "# Storage for fake generations\n",
        "fake_Xs = capacity * [np.zeros((1, img_size, img_size, 3), dtype=np.float32)]\n",
        "fake_Ys = capacity * [np.zeros((1, img_size, img_size, 3), dtype=np.float32)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M_Hxi1eXq7s",
        "colab_type": "text"
      },
      "source": [
        "# Batch Generator\n",
        "\n",
        "Finally, we're almost ready to train.  We just need data!  The most important part!  I've included two kinds of batch generators to help you get data into your CycleGAN network.  One takes your X and Y image collections as arrays.  The other takes a single image for X and Y and will randomly crop it.  I've successfully used this network with very large images, including Hieronymous Bosch's Garden of Earthly Delights.  The first collection was a sketch rendering, and the second was a high resolution image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4qVBwfTXq7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cadl.cycle_gan import batch_generator_dataset, batch_generator_random_crop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OKP61eyXq7u",
        "colab_type": "text"
      },
      "source": [
        "To use the dataset generator, feed in two arrays images shaped: `N` x `H` x `W` x 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UxKla9NXq7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load your data into imgs1 and imgs2 here!\n",
        "# I've loaded in random noise as an example, but you'll want to use\n",
        "# plt.imread or skimage to load in images into a list of images\n",
        "ds_X, ds_Y = np.random.rand(10, img_size, img_size, 3), \\\n",
        "             np.random.rand(10, img_size, img_size, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AJeJVdbXq7x",
        "colab_type": "text"
      },
      "source": [
        "Now you can get batches into your CycleGAN network using the `batch_generator_dataset` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2gOzDdEXq7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_i, Y_i = next(batch_generator_dataset(ds_X, ds_Y))\n",
        "X_i.shape, Y_i.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPJ3EkAUXq70",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, you can grab random crops of a larger image using the `batch_generator_random_crop` function and feed these into your network instead.  You'll want to set the `min_size` and `max_size` parameters to determine what can be cropped and what the crop should be reshaped to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqePBiMiXq70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds_X, ds_Y = np.random.rand(1024, 1024, 3), np.random.rand(1024, 1024, 3)\n",
        "X_i, Y_i = next(batch_generator_random_crop(\n",
        "        ds_X, ds_Y, min_size=img_size, max_size=512))\n",
        "X_i.shape, Y_i.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJsHdPAmXq73",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "The CADL `cycle_gan` module includes a train function.  But if you're curious to know the details of training, I've commented the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCAN_W1FXq73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 0\n",
        "it_i = 0\n",
        "n_epochs = 10\n",
        "ckpt_path = './'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EX4ggyjXq74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train\n",
        "with tf.Session() as sess:\n",
        "    # Build an init op for our variables\n",
        "    init_op = tf.group(tf.global_variables_initializer(),\n",
        "                       tf.local_variables_initializer())\n",
        "    sess.run(init_op)\n",
        "    \n",
        "    # We'll also save our model so we can load it up again\n",
        "    saver = tf.train.Saver()\n",
        "    writer = tf.summary.FileWriter(ckpt_path)\n",
        "    \n",
        "    for epoch_i in range(n_epochs):\n",
        "        # You'll want to use the approriate batch generator here!\n",
        "        for X, Y in batch_generator_random_crop(ds_X, ds_Y):\n",
        "\n",
        "            # First generate in both directions\n",
        "            X_fake, Y_fake = sess.run(\n",
        "                [net['X_fake'], net['Y_fake']],\n",
        "                feed_dict={net['X_real']: X,\n",
        "                           net['Y_real']: Y})\n",
        "\n",
        "            # Now sample from history\n",
        "            if it_i < capacity:\n",
        "                # Not enough samples yet, fill up history buffer\n",
        "                fake_Xs[idx] = X_fake\n",
        "                fake_Ys[idx] = Y_fake\n",
        "                idx = (idx + 1) % capacity\n",
        "            elif np.random.random() > 0.5:\n",
        "                # Swap out a random idx from history\n",
        "                rand_idx = np.random.randint(0, capacity)\n",
        "                fake_Xs[rand_idx], X_fake = X_fake, fake_Xs[rand_idx]\n",
        "                fake_Ys[rand_idx], Y_fake = Y_fake, fake_Ys[rand_idx]\n",
        "            else:\n",
        "                # Use current generation\n",
        "                pass\n",
        "\n",
        "            # Optimize G Networks\n",
        "            loss_G = sess.run(\n",
        "                [net['loss_G'], G],\n",
        "                feed_dict={\n",
        "                    net['X_real']: X,\n",
        "                    net['Y_real']: Y,\n",
        "                    net['Y_fake_sample']: Y_fake,\n",
        "                    net['X_fake_sample']: X_fake\n",
        "                })[0]\n",
        "\n",
        "            # Optimize D_Y\n",
        "            loss_D_Y = sess.run(\n",
        "                [net['loss_D_Y'], D_Y],\n",
        "                feed_dict={\n",
        "                    net['X_real']: X,\n",
        "                    net['Y_real']: Y,\n",
        "                    net['Y_fake_sample']: Y_fake\n",
        "                })[0]\n",
        "\n",
        "            # Optimize D_X\n",
        "            loss_D_X = sess.run(\n",
        "                [net['loss_D_X'], D_X],\n",
        "                feed_dict={\n",
        "                    net['X_real']: X,\n",
        "                    net['Y_real']: Y,\n",
        "                    net['X_fake_sample']: X_fake\n",
        "                })[0]\n",
        "\n",
        "            print(it_i, 'G:', loss_G, 'D_X:', loss_D_X, 'D_Y:', loss_D_Y)\n",
        "\n",
        "            # Update summaries\n",
        "            if it_i % 100 == 0:\n",
        "                summary = sess.run(\n",
        "                    net['summaries'],\n",
        "                    feed_dict={\n",
        "                        net['X_real']: X,\n",
        "                        net['Y_real']: Y,\n",
        "                        net['X_fake_sample']: X_fake,\n",
        "                        net['Y_fake_sample']: Y_fake\n",
        "                    })\n",
        "                writer.add_summary(summary, it_i)\n",
        "            it_i += 1\n",
        "\n",
        "        # Save\n",
        "        if epoch_i % 50 == 0:\n",
        "            saver.save(\n",
        "                sess,\n",
        "                os.path.join(ckpt_path, 'model.ckpt'),\n",
        "                global_step=epoch_i)\n",
        "            \n",
        "        # Show generative images:        \n",
        "        fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "        axs[0][0].set_title('X Real')\n",
        "        axs[0][0].imshow(np.clip(X[0], 0.0, 1.0))\n",
        "        axs[0][1].set_title('X Fake')\n",
        "        axs[0][1].imshow(np.clip(X_fake[0], 0.0, 1.0))\n",
        "        axs[1][0].set_title('Y')\n",
        "        axs[1][0].imshow(np.clip(Y[0], 0.0, 1.0))\n",
        "        axs[1][1].set_title('Y Fake')\n",
        "        axs[1][1].imshow(np.clip(Y_fake[0], 0.0, 1.0))\n",
        "        fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGCEp4OEXq76",
        "colab_type": "text"
      },
      "source": [
        "# Examples\n",
        "\n",
        "Of course, training a CycleGAN on random data isn't nearly as fun as real data!  Here are a few example ideas to get you started:\n",
        "\n",
        "## Tile/Terrain Generation\n",
        "\n",
        "A classic image generation problem is turning labeled images into real life ones.  For instance, we may have a labeled image of a street scene depicting roads, sidewalks, lakes, etc... and want it to fill in all our low poly labels into a rich high resolution texture.  We can explore this process using Google Maps for instance, since it provides high quality textures of streets and satellite imagery.  The resulting CycleGAN looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or3BQkmQaIB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/terrain-generation.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7L4IbP8aIfI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Character/Hero Generation\n",
        "\n",
        "We experimented with an image collection of Pokemon and another image collection of 8-bit low resolution heros.  By applying the encoder model on the low resolution collection, we expect to get high resolution, Pokemon-esque images.  For the reverse process, we expect to turn our Pokemon images into 8-bit renderings.  Of course this is simple enough to do without Deep Learning, so the first process is the more interesting one.  Interestingly, the colors tend to completely change.  Though, there are likely some extensions to the loss that we could add to help enforce that colors stay more constant, and these have been explored in the literature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR2Ut2nEaLbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "plt.imshow(plt.imread('CycleGAN/imgs/character-generation.png'))\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iun_ojUeaMux",
        "colab_type": "text"
      },
      "source": [
        "## Plenty More\n",
        "\n",
        "The authors of the CycleGAN paper offer plenty more ideas on their GitHub along with their own implementation of CycleGAN using Torch and Pytorch: [https://junyanz.github.io/CycleGAN/](https://junyanz.github.io/CycleGAN/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkg7gosgXq77",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Was this useful or did you make something awesome with CycleGAN?  Share it with me at https://twitter.com/pkmital - I'd love to hear!\n",
        "\n",
        "Also, if you are interested in learning more about these networks and related techniques, including Seq2Seq, DRAW, MDN, WaveNet, and plenty more at https://www.kadenze.com/programs/creative-applications-of-deep-learning-with-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Tkw1dJ0Yit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}